# –ì–ª–∞–≤–∞ 5: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è

[‚¨ÖÔ∏è –ü—Ä–µ–¥—ã–¥—É—â–∞—è –≥–ª–∞–≤–∞](./04-rag.md) | [üè† –ù–∞ –≥–ª–∞–≤–Ω—É—é](../../README.md) | [üìë –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ](../TOC.md) | [‚û°Ô∏è –°–ª–µ–¥—É—é—â–∞—è –≥–ª–∞–≤–∞](./06-ux.md)

---

## Backend Proxy vs Direct Client

### Backend Proxy (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```
Frontend ‚Üí Backend Proxy ‚Üí LLM API
         (–ø—É–±–ª–∏—á–Ω—ã–π)    (–ø—Ä–∏–≤–∞—Ç–Ω—ã–π)
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ API –∫–ª—é—á–∏ –∑–∞—â–∏—â–µ–Ω—ã
- ‚úÖ –ö–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø–∞ –∏ rate limiting
- ‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
- ‚úÖ –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- ‚úÖ Cost control

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞
- ‚ùå –í–æ–∑–º–æ–∂–Ω–∞—è latency
- ‚ùå –°–ª–æ–∂–Ω–æ—Å—Ç—å deployment

```typescript
// Backend (Express)
app.post('/api/chat', authenticateUser, async (req, res) => {
  const { messages } = req.body;
  const userId = req.user.id;

  // –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
  if (await isRateLimited(userId)) {
    return res.status(429).json({ error: 'Rate limit exceeded' });
  }

  // –ú–æ–¥–µ—Ä–∞—Ü–∏—è –≤—Ö–æ–¥—è—â–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
  const isClean = await moderateContent(messages);
  if (!isClean) {
    return res.status(400).json({ error: 'Content moderation failed' });
  }

  // –í—ã–∑—ã–≤–∞–µ–º LLM API
  const response = await openai.chat.completions.create({
    model: 'gpt-4-turbo-preview',
    messages: messages,
  });

  // –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ billing
  await logUsage(userId, response.usage);

  res.json({ message: response.choices[0].message.content });
});
```

### Direct Client

```
Frontend ‚Üí LLM API
         (—Å API –∫–ª—é—á–æ–º)
```

**‚ö†Ô∏è –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è production!**

**–í–æ–∑–º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ –¥–ª—è:**
- –ü—Ä–æ—Ç–æ—Ç–∏–ø—ã –∏ MVP
- –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Ollama)
- Serverless functions —Å edge runtime

---

## –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏

### –ß—Ç–æ —Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ

**Backend (Node.js/Express):**
- API –∫–ª—é—á–∏ –∏ —Å–µ–∫—Ä–µ—Ç—ã
- –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
- Rate limiting logic
- –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- Billing –∏ usage tracking
- –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–∏—Å—Ç–æ—Ä–∏—è, user preferences)
- –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ë–î (embeddings)

### –ß—Ç–æ —Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞ –∫–ª–∏–µ–Ω—Ç–µ

**Frontend (React):**
- UI —Å–æ—Å—Ç–æ—è–Ω–∏–µ
- –¢–µ–∫—É—â–∞—è —Å–µ—Å—Å–∏—è —á–∞—Ç–∞
- –õ–æ–∫–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (theme, layout)
- –í—Ä–µ–º–µ–Ω–Ω—ã–π cache

### Feature-Sliced Design

```
src/
‚îú‚îÄ‚îÄ app/                    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îî‚îÄ‚îÄ App.tsx
‚îú‚îÄ‚îÄ pages/                  # –°—Ç—Ä–∞–Ω–∏—Ü—ã
‚îÇ   ‚îî‚îÄ‚îÄ chat/
‚îÇ       ‚îî‚îÄ‚îÄ ChatPage.tsx
‚îú‚îÄ‚îÄ widgets/                # –ö—Ä—É–ø–Ω—ã–µ UI –±–ª–æ–∫–∏
‚îÇ   ‚îî‚îÄ‚îÄ chat-container/
‚îÇ       ‚îú‚îÄ‚îÄ ui/
‚îÇ       ‚îî‚îÄ‚îÄ model/
‚îú‚îÄ‚îÄ features/               # –§–∏—á–∏ (business logic)
‚îÇ   ‚îú‚îÄ‚îÄ send-message/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ rag-search/
‚îÇ       ‚îú‚îÄ‚îÄ ui/
‚îÇ       ‚îú‚îÄ‚îÄ model/
‚îÇ       ‚îî‚îÄ‚îÄ api/
‚îú‚îÄ‚îÄ entities/               # –ë–∏–∑–Ω–µ—Å-—Å—É—â–Ω–æ—Å—Ç–∏
‚îÇ   ‚îú‚îÄ‚îÄ message/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model/
‚îÇ   ‚îî‚îÄ‚îÄ user/
‚îÇ       ‚îú‚îÄ‚îÄ ui/
‚îÇ       ‚îî‚îÄ‚îÄ model/
‚îî‚îÄ‚îÄ shared/                 # –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –∫–æ–¥
    ‚îú‚îÄ‚îÄ api/
    ‚îÇ   ‚îú‚îÄ‚îÄ llm/
    ‚îÇ   ‚îî‚îÄ‚îÄ base.ts
    ‚îú‚îÄ‚îÄ ui/
    ‚îÇ   ‚îú‚îÄ‚îÄ Button/
    ‚îÇ   ‚îî‚îÄ‚îÄ Input/
    ‚îú‚îÄ‚îÄ lib/
    ‚îÇ   ‚îî‚îÄ‚îÄ hooks/
    ‚îî‚îÄ‚îÄ config/
```

**–ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ–∏—á–∏:**

```typescript
// src/features/send-message/api/sendMessageApi.ts
export async function sendMessage(content: string) {
  return fetch('/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ message: content }),
  }).then(res => res.json());
}

// src/features/send-message/model/useSendMessage.ts
import { useMutation } from '@tanstack/react-query';
import { sendMessage } from '../api/sendMessageApi';

export function useSendMessage() {
  return useMutation({
    mutationFn: sendMessage,
    onSuccess: (data) => {
      // –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    },
  });
}

// src/features/send-message/ui/SendMessageButton.tsx
import { useSendMessage } from '../model/useSendMessage';

export function SendMessageButton({ message }: { message: string }) {
  const { mutate, isPending } = useSendMessage();
  
  return (
    <button onClick={() => mutate(message)} disabled={isPending}>
      –û—Ç–ø—Ä–∞–≤–∏—Ç—å
    </button>
  );
}
```

---

## –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞

### Input Moderation

```typescript
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

async function moderateInput(text: string): Promise<{
  safe: boolean;
  categories: string[];
}> {
  const moderation = await openai.moderations.create({
    input: text,
  });

  const result = moderation.results[0];
  
  const flaggedCategories = Object.entries(result.categories)
    .filter(([_, flagged]) => flagged)
    .map(([category]) => category);

  return {
    safe: !result.flagged,
    categories: flaggedCategories,
  };
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ endpoint
app.post('/api/chat', async (req, res) => {
  const { message } = req.body;

  const moderation = await moderateInput(message);
  
  if (!moderation.safe) {
    return res.status(400).json({
      error: 'Content policy violation',
      categories: moderation.categories,
    });
  }

  // –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É...
});
```

### Output Filtering

```typescript
// –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –æ—Ç–≤–µ—Ç–∞—Ö
const BLOCKED_PATTERNS = [
  /api[_-]?key/i,
  /password/i,
  /secret/i,
  // –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
];

function filterOutput(text: string): string {
  let filtered = text;
  
  for (const pattern of BLOCKED_PATTERNS) {
    filtered = filtered.replace(pattern, '[REDACTED]');
  }
  
  return filtered;
}
```

### OpenAI Moderation API

```typescript
interface ModerationResult {
  flagged: boolean;
  categories: {
    hate: boolean;
    'hate/threatening': boolean;
    'self-harm': boolean;
    sexual: boolean;
    'sexual/minors': boolean;
    violence: boolean;
    'violence/graphic': boolean;
  };
  category_scores: {
    [key: string]: number;
  };
}

async function checkContent(text: string): Promise<ModerationResult> {
  const response = await openai.moderations.create({ input: text });
  return response.results[0];
}
```

---

## –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤

```typescript
import NodeCache from 'node-cache';

const cache = new NodeCache({ 
  stdTTL: 3600, // 1 —á–∞—Å
  checkperiod: 120,
});

async function cachedCompletion(
  messages: ChatMessage[],
  options: any
): Promise<string> {
  // –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
  const cacheKey = createCacheKey(messages, options);
  
  // –ü—Ä–æ–≤–µ—Ä—è–µ–º cache
  const cached = cache.get<string>(cacheKey);
  if (cached) {
    return cached;
  }

  // –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
  const response = await openai.chat.completions.create({
    model: options.model || 'gpt-4-turbo-preview',
    messages: messages,
  });

  const result = response.choices[0].message.content || '';
  
  // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ cache
  cache.set(cacheKey, result);
  
  return result;
}

function createCacheKey(messages: ChatMessage[], options: any): string {
  const data = { messages, options };
  return crypto
    .createHash('sha256')
    .update(JSON.stringify(data))
    .digest('hex');
}
```

### –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

```typescript
// –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–æ—Ä–æ–≥–∏–µ –∏ —Ä–µ–¥–∫–æ –º–µ–Ω—è—é—Ç—Å—è - –∫—ç—à–∏—Ä—É–µ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ
const embeddingCache = new NodeCache({ stdTTL: 86400 * 7 }); // 7 –¥–Ω–µ–π

async function getCachedEmbedding(text: string): Promise<number[]> {
  const cacheKey = `emb:${crypto
    .createHash('md5')
    .update(text)
    .digest('hex')}`;
  
  const cached = embeddingCache.get<number[]>(cacheKey);
  if (cached) return cached;

  const embedding = await getEmbedding(text);
  embeddingCache.set(cacheKey, embedding);
  
  return embedding;
}
```

### CDN –∏ Edge Caching

```typescript
// Vercel Edge Functions
export const config = {
  runtime: 'edge',
};

export default async function handler(req: Request) {
  const { searchParams } = new URL(req.url);
  const query = searchParams.get('q');

  // Edge cache
  const cacheKey = `llm:${query}`;
  const cached = await caches.default.match(cacheKey);
  
  if (cached) {
    return cached;
  }

  const response = await generateResponse(query);
  
  // –ö—ç—à–∏—Ä—É–µ–º –Ω–∞ 1 —á–∞—Å
  const newResponse = new Response(response, {
    headers: {
      'Cache-Control': 'public, max-age=3600',
    },
  });
  
  await caches.default.put(cacheKey, newResponse.clone());
  
  return newResponse;
}
```

---

## Rate Limiting –∏ Billing

### User Rate Limits

```typescript
import rateLimit from 'express-rate-limit';
import RedisStore from 'rate-limit-redis';
import Redis from 'ioredis';

const redis = new Redis(process.env.REDIS_URL);

// –ë–∞–∑–æ–≤—ã–π rate limiter
const limiter = rateLimit({
  store: new RedisStore({
    client: redis,
    prefix: 'rl:',
  }),
  windowMs: 15 * 60 * 1000, // 15 –º–∏–Ω—É—Ç
  max: 100, // –º–∞–∫—Å 100 –∑–∞–ø—Ä–æ—Å–æ–≤
  message: 'Too many requests, please try again later',
});

app.use('/api/', limiter);

// –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π rate limiting —Å —Ç–∞—Ä–∏—Ñ–∞–º–∏
interface UserTier {
  name: string;
  requestsPerHour: number;
  tokensPerDay: number;
}

const TIERS: Record<string, UserTier> = {
  free: { name: 'Free', requestsPerHour: 10, tokensPerDay: 10000 },
  pro: { name: 'Pro', requestsPerHour: 100, tokensPerDay: 100000 },
  enterprise: { name: 'Enterprise', requestsPerHour: 1000, tokensPerDay: 1000000 },
};

async function checkUserLimits(
  userId: string,
  tier: string
): Promise<{ allowed: boolean; remaining: number }> {
  const userTier = TIERS[tier];
  
  // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –≤ —á–∞—Å
  const requestKey = `rl:requests:${userId}:${getCurrentHour()}`;
  const requests = await redis.incr(requestKey);
  await redis.expire(requestKey, 3600);

  if (requests > userTier.requestsPerHour) {
    return { allowed: false, remaining: 0 };
  }

  // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω—ã –≤ –¥–µ–Ω—å
  const tokenKey = `rl:tokens:${userId}:${getCurrentDay()}`;
  const tokens = parseInt(await redis.get(tokenKey) || '0');

  if (tokens > userTier.tokensPerDay) {
    return { allowed: false, remaining: 0 };
  }

  return {
    allowed: true,
    remaining: userTier.requestsPerHour - requests,
  };
}
```

### Cost Tracking

```typescript
interface UsageLog {
  userId: string;
  timestamp: Date;
  model: string;
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
  cost: number;
}

const TOKEN_COSTS = {
  'gpt-4-turbo-preview': { input: 0.01, output: 0.03 },
  'gpt-3.5-turbo': { input: 0.0005, output: 0.0015 },
  'claude-3-opus': { input: 0.015, output: 0.075 },
};

function calculateCost(
  model: string,
  promptTokens: number,
  completionTokens: number
): number {
  const costs = TOKEN_COSTS[model];
  if (!costs) return 0;

  return (
    (promptTokens / 1000) * costs.input +
    (completionTokens / 1000) * costs.output
  );
}

async function logUsage(
  userId: string,
  model: string,
  usage: { prompt_tokens: number; completion_tokens: number }
): Promise<void> {
  const cost = calculateCost(
    model,
    usage.prompt_tokens,
    usage.completion_tokens
  );

  const log: UsageLog = {
    userId,
    timestamp: new Date(),
    model,
    promptTokens: usage.prompt_tokens,
    completionTokens: usage.completion_tokens,
    totalTokens: usage.prompt_tokens + usage.completion_tokens,
    cost,
  };

  // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
  await db.usageLogs.create(log);

  // –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
  const tokenKey = `rl:tokens:${userId}:${getCurrentDay()}`;
  await redis.incrby(tokenKey, log.totalTokens);
  await redis.expire(tokenKey, 86400);
}
```

### Budget Alerts

```typescript
async function checkBudgetAlert(userId: string): Promise<void> {
  const today = getCurrentDay();
  const usage = await db.usageLogs.aggregate([
    {
      $match: {
        userId,
        timestamp: { $gte: new Date(today) },
      },
    },
    {
      $group: {
        _id: null,
        totalCost: { $sum: '$cost' },
        totalTokens: { $sum: '$totalTokens' },
      },
    },
  ]);

  const totalCost = usage[0]?.totalCost || 0;
  const user = await db.users.findOne({ _id: userId });
  const budget = user.dailyBudget || 10; // $10 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

  if (totalCost >= budget * 0.8) {
    // 80% –æ—Ç –±—é–¥–∂–µ—Ç–∞
    await sendAlert(userId, {
      type: 'budget_warning',
      message: `You've used 80% of your daily budget ($${totalCost.toFixed(2)} / $${budget})`,
    });
  }

  if (totalCost >= budget) {
    // –ü—Ä–µ–≤—ã—à–µ–Ω –±—é–¥–∂–µ—Ç
    await disableUserAccess(userId);
    await sendAlert(userId, {
      type: 'budget_exceeded',
      message: `Daily budget exceeded. Access temporarily disabled.`,
    });
  }
}
```

---

## Error Handling

### Retry Strategies

```typescript
interface RetryConfig {
  maxRetries: number;
  initialDelay: number;
  maxDelay: number;
  backoffMultiplier: number;
  retryableErrors: string[];
}

const DEFAULT_RETRY_CONFIG: RetryConfig = {
  maxRetries: 3,
  initialDelay: 1000,
  maxDelay: 10000,
  backoffMultiplier: 2,
  retryableErrors: [
    'ECONNRESET',
    'ETIMEDOUT',
    'ENOTFOUND',
    'rate_limit_exceeded',
    'server_error',
  ],
};

async function withRetry<T>(
  fn: () => Promise<T>,
  config: RetryConfig = DEFAULT_RETRY_CONFIG
): Promise<T> {
  let lastError: Error;
  let delay = config.initialDelay;

  for (let attempt = 0; attempt <= config.maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;
      
      const isRetryable = config.retryableErrors.some(
        err => lastError.message.includes(err)
      );

      if (!isRetryable || attempt === config.maxRetries) {
        throw lastError;
      }

      console.log(`Retry attempt ${attempt + 1}/${config.maxRetries} after ${delay}ms`);
      
      await sleep(delay);
      delay = Math.min(delay * config.backoffMultiplier, config.maxDelay);
    }
  }

  throw lastError!;
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
const response = await withRetry(() =>
  openai.chat.completions.create({
    model: 'gpt-4-turbo-preview',
    messages: messages,
  })
);
```

### Exponential Backoff

```typescript
class ExponentialBackoff {
  private attempt = 0;

  constructor(
    private config = {
      initialDelay: 1000,
      maxDelay: 32000,
      factor: 2,
      jitter: true,
    }
  ) {}

  async execute<T>(fn: () => Promise<T>): Promise<T> {
    try {
      const result = await fn();
      this.attempt = 0; // Reset on success
      return result;
    } catch (error) {
      this.attempt++;
      const delay = this.calculateDelay();
      
      console.log(`Backing off for ${delay}ms (attempt ${this.attempt})`);
      await sleep(delay);
      
      return this.execute(fn);
    }
  }

  private calculateDelay(): number {
    const delay = Math.min(
      this.config.initialDelay * Math.pow(this.config.factor, this.attempt - 1),
      this.config.maxDelay
    );

    // –î–æ–±–∞–≤–ª—è–µ–º jitter –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è thundering herd
    if (this.config.jitter) {
      return delay * (0.5 + Math.random() * 0.5);
    }

    return delay;
  }
}
```

### Circuit Breaker Pattern

```typescript
enum CircuitState {
  CLOSED = 'CLOSED',   // –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞
  OPEN = 'OPEN',       // –°–ª–æ–º–∞–Ω–æ, –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã
  HALF_OPEN = 'HALF_OPEN', // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—á–∏–Ω–∏–ª–æ—Å—å –ª–∏
}

class CircuitBreaker {
  private state = CircuitState.CLOSED;
  private failureCount = 0;
  private successCount = 0;
  private nextAttempt = Date.now();

  constructor(
    private config = {
      failureThreshold: 5,
      successThreshold: 2,
      timeout: 60000, // 1 –º–∏–Ω—É—Ç–∞
    }
  ) {}

  async execute<T>(fn: () => Promise<T>): Promise<T> {
    if (this.state === CircuitState.OPEN) {
      if (Date.now() < this.nextAttempt) {
        throw new Error('Circuit breaker is OPEN');
      }
      this.state = CircuitState.HALF_OPEN;
    }

    try {
      const result = await fn();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }

  private onSuccess(): void {
    this.failureCount = 0;

    if (this.state === CircuitState.HALF_OPEN) {
      this.successCount++;
      
      if (this.successCount >= this.config.successThreshold) {
        this.state = CircuitState.CLOSED;
        this.successCount = 0;
      }
    }
  }

  private onFailure(): void {
    this.failureCount++;
    this.successCount = 0;

    if (this.failureCount >= this.config.failureThreshold) {
      this.state = CircuitState.OPEN;
      this.nextAttempt = Date.now() + this.config.timeout;
    }
  }

  getState(): CircuitState {
    return this.state;
  }
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
const breaker = new CircuitBreaker();

try {
  const response = await breaker.execute(() =>
    openai.chat.completions.create({ ... })
  );
} catch (error) {
  if (error.message === 'Circuit breaker is OPEN') {
    // –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –∏–ª–∏ –∫—ç—à
  }
}
```

---

## Batching

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å batching

‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ batching –¥–ª—è:**
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤
- –ú–æ–¥–µ—Ä–∞—Ü–∏—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
- Batch inference –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏

```typescript
async function batchEmbeddings(texts: string[]): Promise<number[][]> {
  const BATCH_SIZE = 2048; // OpenAI –ª–∏–º–∏—Ç
  const batches: string[][] = [];
  
  // –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
  for (let i = 0; i < texts.length; i += BATCH_SIZE) {
    batches.push(texts.slice(i, i + BATCH_SIZE));
  }

  // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
  const results = await Promise.all(
    batches.map(async (batch) => {
      const response = await openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: batch,
      });
      return response.data.map(d => d.embedding);
    })
  );

  // –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
  return results.flat();
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
const documents = ['text1', 'text2', ..., 'text1000'];
const embeddings = await batchEmbeddings(documents);
```

### Rate-limited batching

```typescript
class RateLimitedBatcher<T, R> {
  private queue: Array<{
    item: T;
    resolve: (value: R) => void;
    reject: (error: Error) => void;
  }> = [];
  private processing = false;

  constructor(
    private batchFn: (items: T[]) => Promise<R[]>,
    private config = {
      batchSize: 50,
      batchDelay: 100,
      maxConcurrent: 5,
    }
  ) {}

  async add(item: T): Promise<R> {
    return new Promise((resolve, reject) => {
      this.queue.push({ item, resolve, reject });
      
      if (!this.processing) {
        this.process();
      }
    });
  }

  private async process(): Promise<void> {
    this.processing = true;

    while (this.queue.length > 0) {
      const batch = this.queue.splice(0, this.config.batchSize);
      
      try {
        const results = await this.batchFn(
          batch.map(b => b.item)
        );
        
        batch.forEach((b, i) => b.resolve(results[i]));
      } catch (error) {
        batch.forEach(b => b.reject(error as Error));
      }

      // –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
      if (this.queue.length > 0) {
        await sleep(this.config.batchDelay);
      }
    }

    this.processing = false;
  }
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
const batcher = new RateLimitedBatcher(
  async (texts: string[]) => {
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: texts,
    });
    return response.data.map(d => d.embedding);
  }
);

// –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–∞—Ç—á–∏—Ç—Å—è)
const embedding1 = await batcher.add('text 1');
const embedding2 = await batcher.add('text 2');
```

---

## –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ

```typescript
// –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Redis –¥–ª—è shared state –º–µ–∂–¥—É –∏–Ω—Å—Ç–∞–Ω—Å–∞–º–∏
import Redis from 'ioredis';
import { Queue } from 'bullmq';

const redis = new Redis(process.env.REDIS_URL);

// –û—á–µ—Ä–µ–¥—å –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
const llmQueue = new Queue('llm-jobs', {
  connection: redis,
});

// Worker (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ)
const worker = new Worker('llm-jobs', async (job) => {
  const { messages, userId } = job.data;
  
  const response = await openai.chat.completions.create({
    model: 'gpt-4-turbo-preview',
    messages: messages,
  });

  // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
  await saveResponse(userId, response);
  
  return response;
}, {
  connection: redis,
  concurrency: 10, // –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
});

// API endpoint –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ –æ—á–µ—Ä–µ–¥—å
app.post('/api/chat', async (req, res) => {
  const job = await llmQueue.add('chat', {
    messages: req.body.messages,
    userId: req.user.id,
  });

  res.json({ jobId: job.id });
});

// Endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞
app.get('/api/jobs/:id', async (req, res) => {
  const job = await llmQueue.getJob(req.params.id);
  res.json({
    status: await job.getState(),
    result: await job.returnvalue,
  });
});
```

### Load Balancing

```typescript
// –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
class LoadBalancer {
  private providers = [
    { name: 'openai', weight: 70, available: true },
    { name: 'anthropic', weight: 30, available: true },
  ];

  selectProvider(): string {
    const available = this.providers.filter(p => p.available);
    const totalWeight = available.reduce((sum, p) => sum + p.weight, 0);
    
    let random = Math.random() * totalWeight;
    
    for (const provider of available) {
      random -= provider.weight;
      if (random <= 0) {
        return provider.name;
      }
    }

    return available[0].name;
  }

  markUnavailable(name: string): void {
    const provider = this.providers.find(p => p.name === name);
    if (provider) {
      provider.available = false;
      
      // –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É
      setTimeout(() => {
        provider.available = true;
      }, 60000);
    }
  }
}
```

---

## –†–µ–∑—é–º–µ –≥–ª–∞–≤—ã

–í —ç—Ç–æ–π –≥–ª–∞–≤–µ –≤—ã —É–∑–Ω–∞–ª–∏:
- ‚úÖ –ü–æ—á–µ–º—É –Ω—É–∂–µ–Ω backend proxy (–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –∫–æ–Ω—Ç—Ä–æ–ª—å)
- ‚úÖ –ö–∞–∫ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ Feature-Sliced Design
- ‚úÖ –ú–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –∑–∞—â–∏—Ç–∞ –æ—Ç abuse
- ‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –∑–∞—Ç—Ä–∞—Ç
- ‚úÖ Rate limiting –∏ cost tracking
- ‚úÖ Retry strategies –∏ Circuit Breaker
- ‚úÖ Batching –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ß—Ç–æ –¥–∞–ª—å—à–µ?

–í —Å–ª–µ–¥—É—é—â–µ–π –≥–ª–∞–≤–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º UX-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–¥–æ–±–Ω—ã—Ö AI-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤.

---

[‚¨ÖÔ∏è –ì–ª–∞–≤–∞ 4: RAG](./04-rag.md) | [üè† –ù–∞ –≥–ª–∞–≤–Ω—É—é](../../README.md) | [üìë –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ](../TOC.md) | [‚û°Ô∏è –ì–ª–∞–≤–∞ 6: UX](./06-ux.md)
