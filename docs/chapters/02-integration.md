# –ì–ª–∞–≤–∞ 2: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ API

[‚¨ÖÔ∏è –ü—Ä–µ–¥—ã–¥—É—â–∞—è –≥–ª–∞–≤–∞](./01-basics.md) | [üè† –ù–∞ –≥–ª–∞–≤–Ω—É—é](../../README.md) | [üìë –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ](../TOC.md) | [‚û°Ô∏è –°–ª–µ–¥—É—é—â–∞—è –≥–ª–∞–≤–∞](./03-streaming.md)

---

## –í–≤–µ–¥–µ–Ω–∏–µ

–í —ç—Ç–æ–π –≥–ª–∞–≤–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM API –≤ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ. –í—ã –Ω–∞—É—á–∏—Ç–µ—Å—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å OpenAI, Anthropic, Cohere –∏ –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç–∞–Ω—Å–∞–º–∏.

---

## OpenAI API

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ SDK

```bash
# npm
npm install openai

# yarn
yarn add openai

# pnpm
pnpm add openai
```

### –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è

```typescript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY, // –ù–∏–∫–æ–≥–¥–∞ –Ω–µ —Ö—Ä–∞–Ω–∏—Ç–µ –∫–ª—é—á–∏ –≤ –∫–æ–¥–µ!
  dangerouslyAllowBrowser: false, // –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
});
```

**‚ö†Ô∏è –í–∞–∂–Ω–æ:** –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ API –∫–ª—é—á–∏ –Ω–∞–ø—Ä—è–º—É—é –≤ –±—Ä–∞—É–∑–µ—Ä–µ! –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ backend proxy.

### –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

```typescript
interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

async function generateCompletion(
  messages: ChatMessage[]
): Promise<string> {
  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4-turbo-preview',
      messages: messages,
      temperature: 0.7, // –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (0-2)
      max_tokens: 1000, // –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
      top_p: 1, // Nucleus sampling
      frequency_penalty: 0, // –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
      presence_penalty: 0, // –®—Ç—Ä–∞—Ñ –∑–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Ç–µ–º
    });

    return response.choices[0].message.content || '';
  } catch (error) {
    console.error('OpenAI API Error:', error);
    throw error;
  }
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
const messages: ChatMessage[] = [
  { role: 'system', content: '–¢—ã helpful assistant –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤' },
  { role: 'user', content: '–û–±—ä—è—Å–Ω–∏ React.memo' },
];

const answer = await generateCompletion(messages);
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞

#### Temperature (0-2)
–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤:
- **0.0-0.3**: –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã
- **0.7-0.9**: –ö—Ä–µ–∞—Ç–∏–≤–Ω—ã–µ, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
- **1.5-2.0**: –û—á–µ–Ω—å –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–µ, –∏–Ω–æ–≥–¥–∞ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ

```typescript
// –î–ª—è –∫–æ–¥–∞ –∏ —Ñ–∞–∫—Ç–æ–≤
const codeResponse = await openai.chat.completions.create({
  model: 'gpt-4-turbo-preview',
  messages: [{ role: 'user', content: '–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏' }],
  temperature: 0.2, // –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º–∞
});

// –î–ª—è –∫—Ä–µ–∞—Ç–∏–≤–∞
const creativeResponse = await openai.chat.completions.create({
  model: 'gpt-4-turbo-preview',
  messages: [{ role: 'user', content: '–ü—Ä–∏–¥—É–º–∞–π –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–∞' }],
  temperature: 1.2, // –í—ã—Å–æ–∫–∞—è –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
});
```

#### JSON Mode

```typescript
const response = await openai.chat.completions.create({
  model: 'gpt-4-turbo-preview',
  messages: [
    { role: 'system', content: '–¢—ã –≤–æ–∑–≤—Ä–∞—â–∞–µ—à—å –¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ' },
    { role: 'user', content: '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ React' }
  ],
  response_format: { type: 'json_object' },
});

const data = JSON.parse(response.choices[0].message.content);
```

#### Function Calling

```typescript
const tools = [
  {
    type: 'function',
    function: {
      name: 'get_weather',
      description: '–ü–æ–ª—É—á–∏—Ç—å –ø–æ–≥–æ–¥—É –¥–ª—è –≥–æ—Ä–æ–¥–∞',
      parameters: {
        type: 'object',
        properties: {
          city: { type: 'string', description: '–ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞' },
          units: { type: 'string', enum: ['celsius', 'fahrenheit'] },
        },
        required: ['city'],
      },
    },
  },
];

const response = await openai.chat.completions.create({
  model: 'gpt-4-turbo-preview',
  messages: [{ role: 'user', content: '–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ?' }],
  tools: tools,
  tool_choice: 'auto',
});

// –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—ã–∑–≤–∞–ª–∞ —Ñ—É–Ω–∫—Ü–∏—é
if (response.choices[0].message.tool_calls) {
  const toolCall = response.choices[0].message.tool_calls[0];
  const args = JSON.parse(toolCall.function.arguments);
  // –í—ã–∑—ã–≤–∞–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
  const weatherData = await getWeather(args.city);
}
```

---

## Anthropic Claude API

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ SDK

```bash
npm install @anthropic-ai/sdk
```

### –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```typescript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

async function chatWithClaude(prompt: string): Promise<string> {
  const message = await anthropic.messages.create({
    model: 'claude-3-sonnet-20240229',
    max_tokens: 1024,
    messages: [
      { role: 'user', content: prompt }
    ],
  });

  return message.content[0].type === 'text' 
    ? message.content[0].text 
    : '';
}
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Claude

#### System Prompts

```typescript
const message = await anthropic.messages.create({
  model: 'claude-3-sonnet-20240229',
  max_tokens: 1024,
  system: '–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ TypeScript. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.',
  messages: [
    { role: 'user', content: '–ß—Ç–æ —Ç–∞–∫–æ–µ generics?' }
  ],
});
```

#### Thinking Tags

Claude –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≤–æ–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è:

```typescript
const message = await anthropic.messages.create({
  model: 'claude-3-opus-20240229',
  max_tokens: 2048,
  messages: [
    { 
      role: 'user', 
      content: '–†–µ—à–∏ –∑–∞–¥–∞—á—É: <thinking>–ø–æ–∫–∞–∂–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è</thinking> 2x + 5 = 15' 
    }
  ],
});
```

---

## Cohere API

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ SDK

```bash
npm install cohere-ai
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```typescript
import { CohereClient } from 'cohere-ai';

const cohere = new CohereClient({
  token: process.env.COHERE_API_KEY,
});

async function generateWithCohere(prompt: string): Promise<string> {
  const response = await cohere.generate({
    model: 'command',
    prompt: prompt,
    max_tokens: 300,
    temperature: 0.9,
  });

  return response.generations[0].text;
}

// Chat API
async function chatWithCohere(message: string): Promise<string> {
  const response = await cohere.chat({
    message: message,
    model: 'command',
  });

  return response.text;
}
```

---

## –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç–∞–Ω—Å—ã

### Ollama

[Ollama](https://ollama.ai/) ‚Äî —Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –∑–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ.

#### –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# –°–∫–∞—á–∞–π—Ç–µ —Å https://ollama.ai/download
```

#### –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏

```bash
# –°–∫–∞—á–∞—Ç—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å Llama 3
ollama run llama3

# –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
ollama list

# –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å –±–µ–∑ –∑–∞–ø—É—Å–∫–∞
ollama pull mistral
```

#### API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

```typescript
interface OllamaMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

async function chatWithOllama(
  messages: OllamaMessage[]
): Promise<string> {
  const response = await fetch('http://localhost:11434/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model: 'llama3',
      messages: messages,
      stream: false,
    }),
  });

  const data = await response.json();
  return data.message.content;
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
const response = await chatWithOllama([
  { role: 'user', content: '–û–±—ä—è—Å–Ω–∏ –∑–∞–º—ã–∫–∞–Ω–∏—è –≤ JavaScript' }
]);
```

### Replicate

[Replicate](https://replicate.com/) ‚Äî –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ API.

```bash
npm install replicate
```

```typescript
import Replicate from 'replicate';

const replicate = new Replicate({
  auth: process.env.REPLICATE_API_TOKEN,
});

async function runLlama(prompt: string): Promise<string> {
  const output = await replicate.run(
    'meta/llama-2-70b-chat:latest',
    {
      input: {
        prompt: prompt,
        max_length: 500,
      }
    }
  );

  return Array.isArray(output) ? output.join('') : String(output);
}
```

### –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ llama.cpp

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
./server -m models/llama-2-7b-chat.Q4_K_M.gguf --port 8080
```

```typescript
async function chatWithLocalLlama(prompt: string): Promise<string> {
  const response = await fetch('http://localhost:8080/completion', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      prompt: prompt,
      n_predict: 400,
      temperature: 0.7,
    }),
  });

  const data = await response.json();
  return data.content;
}
```

---

## –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

### –•—Ä–∞–Ω–µ–Ω–∏–µ API –∫–ª—é—á–µ–π

#### ‚ùå –ù–∏–∫–æ–≥–¥–∞ —Ç–∞–∫ –Ω–µ –¥–µ–ª–∞–π—Ç–µ

```typescript
// –ü–õ–û–•–û: –∫–ª—é—á –≤ –∫–æ–¥–µ
const openai = new OpenAI({
  apiKey: 'sk-proj-abc123...',
});

// –ü–õ–û–•–û: –∫–ª—é—á –≤ –±—Ä–∞—É–∑–µ—Ä–µ
const apiKey = 'sk-proj-abc123...';
fetch(`https://api.openai.com/v1/chat/completions`, {
  headers: { 'Authorization': `Bearer ${apiKey}` }
});
```

#### ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥

**1. Environment Variables**

```bash
# .env
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
COHERE_API_KEY=...
```

```typescript
// .env.example (–∫–æ–º–º–∏—Ç–∏–º –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π)
OPENAI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here
```

```typescript
// Backend (Node.js)
import dotenv from 'dotenv';
dotenv.config();

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});
```

**2. Secret Management (Production)**

```typescript
// –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ secret manager
import { SecretManagerServiceClient } from '@google-cloud/secret-manager';

const client = new SecretManagerServiceClient();

async function getSecret(name: string): Promise<string> {
  const [version] = await client.accessSecretVersion({
    name: `projects/my-project/secrets/${name}/versions/latest`,
  });
  return version.payload?.data?.toString() || '';
}

const apiKey = await getSecret('openai-api-key');
```

### Environment Variables

```typescript
// src/config/env.ts
interface Environment {
  openaiApiKey: string;
  anthropicApiKey: string;
  nodeEnv: 'development' | 'production' | 'test';
}

function validateEnv(): Environment {
  const required = ['OPENAI_API_KEY'];
  
  for (const key of required) {
    if (!process.env[key]) {
      throw new Error(`Missing required environment variable: ${key}`);
    }
  }

  return {
    openaiApiKey: process.env.OPENAI_API_KEY!,
    anthropicApiKey: process.env.ANTHROPIC_API_KEY || '',
    nodeEnv: (process.env.NODE_ENV as any) || 'development',
  };
}

export const env = validateEnv();
```

### Proxy –¥–ª—è —Å–∫—Ä—ã—Ç–∏—è –∫–ª—é—á–µ–π

–í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ backend proxy –º–µ–∂–¥—É —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º –∏ LLM API:

```
Frontend ‚Üí Backend Proxy ‚Üí LLM API
         (no API keys)    (API keys stored securely)
```

---

## CORS –∏ –ø—Ä–æ–∫—Å–∏

### –ü—Ä–æ–±–ª–µ–º–∞ CORS

–ú–Ω–æ–≥–∏–µ LLM API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –ø—Ä—è–º—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞ –∏–∑-–∑–∞ CORS:

```typescript
// ‚ùå –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞
fetch('https://api.openai.com/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${apiKey}`,
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({ ... }),
});
// Error: CORS policy blocked
```

### Backend Proxy

**–í–∞—Ä–∏–∞–Ω—Ç 1: Express –ø—Ä–æ–∫—Å–∏**

```typescript
// backend/server.ts
import express from 'express';
import OpenAI from 'openai';

const app = express();
app.use(express.json());

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

app.post('/api/chat', async (req, res) => {
  try {
    const { messages } = req.body;
    
    const completion = await openai.chat.completions.create({
      model: 'gpt-4-turbo-preview',
      messages: messages,
    });

    res.json({
      message: completion.choices[0].message.content,
    });
  } catch (error) {
    console.error(error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

app.listen(3001, () => {
  console.log('Proxy server running on http://localhost:3001');
});
```

**–í–∞—Ä–∏–∞–Ω—Ç 2: Next.js API Routes**

```typescript
// app/api/chat/route.ts
import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export async function POST(request: NextRequest) {
  const { messages } = await request.json();

  try {
    const completion = await openai.chat.completions.create({
      model: 'gpt-4-turbo-preview',
      messages: messages,
    });

    return NextResponse.json({
      message: completion.choices[0].message.content,
    });
  } catch (error) {
    return NextResponse.json(
      { error: 'Failed to generate response' },
      { status: 500 }
    );
  }
}
```

### Serverless Functions

**Vercel Functions**

```typescript
// api/chat.ts
import type { VercelRequest, VercelResponse } from '@vercel/node';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export default async function handler(
  req: VercelRequest,
  res: VercelResponse
) {
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  const { messages } = req.body;

  const completion = await openai.chat.completions.create({
    model: 'gpt-4-turbo-preview',
    messages: messages,
  });

  res.json({
    message: completion.choices[0].message.content,
  });
}
```

---

## –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞

### –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π LLM –∫–ª–∏–µ–Ω—Ç

```typescript
// src/shared/api/llm/llmClient.ts

type LLMProvider = 'openai' | 'anthropic' | 'cohere' | 'ollama';

interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface LLMResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

export class LLMClient {
  constructor(
    private provider: LLMProvider,
    private apiKey?: string
  ) {}

  async chat(
    messages: LLMMessage[],
    options?: {
      model?: string;
      temperature?: number;
      maxTokens?: number;
    }
  ): Promise<LLMResponse> {
    switch (this.provider) {
      case 'openai':
        return this.chatOpenAI(messages, options);
      case 'anthropic':
        return this.chatAnthropic(messages, options);
      case 'ollama':
        return this.chatOllama(messages, options);
      default:
        throw new Error(`Unsupported provider: ${this.provider}`);
    }
  }

  private async chatOpenAI(
    messages: LLMMessage[],
    options?: any
  ): Promise<LLMResponse> {
    const openai = new OpenAI({ apiKey: this.apiKey });
    
    const response = await openai.chat.completions.create({
      model: options?.model || 'gpt-4-turbo-preview',
      messages: messages,
      temperature: options?.temperature || 0.7,
      max_tokens: options?.maxTokens || 1000,
    });

    return {
      content: response.choices[0].message.content || '',
      usage: {
        promptTokens: response.usage?.prompt_tokens || 0,
        completionTokens: response.usage?.completion_tokens || 0,
        totalTokens: response.usage?.total_tokens || 0,
      },
    };
  }

  private async chatAnthropic(
    messages: LLMMessage[],
    options?: any
  ): Promise<LLMResponse> {
    // –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è Claude
    throw new Error('Not implemented');
  }

  private async chatOllama(
    messages: LLMMessage[],
    options?: any
  ): Promise<LLMResponse> {
    const response = await fetch('http://localhost:11434/api/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: options?.model || 'llama3',
        messages: messages,
        stream: false,
      }),
    });

    const data = await response.json();
    return {
      content: data.message.content,
    };
  }
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
const client = new LLMClient('openai', process.env.OPENAI_API_KEY);
const response = await client.chat([
  { role: 'user', content: 'Hello!' }
]);
```

### Frontend –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏

```typescript
// src/shared/api/llm/chatApi.ts

interface ChatRequest {
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
  model?: string;
  temperature?: number;
}

interface ChatResponse {
  message: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
  };
}

export async function sendChatMessage(
  request: ChatRequest
): Promise<ChatResponse> {
  const response = await fetch('/api/chat', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(request),
  });

  if (!response.ok) {
    const error = await response.json();
    throw new Error(error.message || 'Failed to send message');
  }

  return response.json();
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–µ
import { sendChatMessage } from '@/shared/api/llm/chatApi';

async function handleSubmit(message: string) {
  try {
    const response = await sendChatMessage({
      messages: [
        { role: 'user', content: message }
      ],
    });
    
    console.log(response.message);
  } catch (error) {
    console.error('Error:', error);
  }
}
```

---

## –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (2025)

### SDK –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [OpenAI Node.js SDK](https://github.com/openai/openai-node) - –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π SDK
- [Anthropic TypeScript SDK](https://github.com/anthropics/anthropic-sdk-typescript)
- [Cohere Node SDK](https://docs.cohere.com/docs/node-sdk)
- [Replicate SDK](https://github.com/replicate/replicate-javascript)

### API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Anthropic API Docs](https://docs.anthropic.com/claude/reference)
- [Cohere API Docs](https://docs.cohere.com/reference/about)

### –õ–æ–∫–∞–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
- [Ollama](https://ollama.ai/) - –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π –ª–æ–∫–∞–ª—å–Ω–æ
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - C++ inference
- [Replicate](https://replicate.com/) - managed hosting

---

## –†–µ–∑—é–º–µ –≥–ª–∞–≤—ã

–í —ç—Ç–æ–π –≥–ª–∞–≤–µ –≤—ã —É–∑–Ω–∞–ª–∏:
- ‚úÖ –ö–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å OpenAI, Anthropic, Cohere API
- ‚úÖ –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (Ollama, Replicate)
- ‚úÖ –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å API –∫–ª—é—á–∏
- ‚úÖ –ó–∞—á–µ–º –Ω—É–∂–µ–Ω backend proxy –∏ –∫–∞–∫ –µ–≥–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å
- ‚úÖ –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π LLM –∫–ª–∏–µ–Ω—Ç

### –ß—Ç–æ –¥–∞–ª—å—à–µ?

–í —Å–ª–µ–¥—É—é—â–µ–π –≥–ª–∞–≤–µ –º—ã –∏–∑—É—á–∏–º –ø–æ—Ç–æ–∫–æ–≤—É—é –æ—Ç–¥–∞—á—É (streaming) ‚Äî –∫–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤—á–∏–≤—ã–π UI —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –æ—Ç–≤–µ—Ç–∞ —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º.

---

[‚¨ÖÔ∏è –ì–ª–∞–≤–∞ 1: –û—Å–Ω–æ–≤—ã LLM](./01-basics.md) | [üè† –ù–∞ –≥–ª–∞–≤–Ω—É—é](../../README.md) | [üìë –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ](../TOC.md) | [‚û°Ô∏è –ì–ª–∞–≤–∞ 3: Streaming](./03-streaming.md)
